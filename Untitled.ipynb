{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This script loads, explores, and visualizes the salary prediction datasets'''\n",
    "   \n",
    "    \n",
    "__author__ = 'DSDJ Team'\n",
    "__email__ = 'info@datasciencedreamjob..com'\n",
    "__website__ = 'www.datasciencedreamjob.com'\n",
    "\n",
    "__copyright__ = 'Copyright 2018, Data Science Dream Job LLC'\n",
    "# it was a course from datascience dream job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(file):\n",
    "    '''loads csv to pd dataframe'''\n",
    "    return pd.read_csv(file)\n",
    "\n",
    "def consolidate_data(df1, df2, key=None, left_index=False, right_index=False):\n",
    "    '''perform inner join to return only records that are present in both dataframes'''\n",
    "    return pd.merge(left=df1, right=df2, how='inner', on=key, left_index=left_index, right_index=right_index)\n",
    "\n",
    "def clean_data(raw_df):\n",
    "    '''remove rows that contain salary <= 0 or duplicate job IDs'''\n",
    "    clean_df = raw_df.drop_duplicates(subset='jobId')\n",
    "    clean_df = clean_df[clean_df.salary>0]\n",
    "    return clean_df\n",
    "\n",
    "def one_hot_encode_feature_df(df, cat_vars=None, num_vars=None):\n",
    "    '''performs one-hot encoding on all categorical variables and combines result with continous variables'''\n",
    "    cat_df = pd.get_dummies(df[cat_vars])\n",
    "    num_df = df[num_vars].apply(pd.to_numeric)\n",
    "    return pd.concat([cat_df, num_df], axis=1)#,ignore_index=False)\n",
    "\n",
    "def get_target_df(df, target):\n",
    "    '''returns target dataframe'''\n",
    "    return df[target]\n",
    "\n",
    "def train_model(model, feature_df, target_df, num_procs, mean_mse, cv_std):\n",
    "    neg_mse = cross_val_score(model, feature_df, target_df, cv=2, n_jobs=num_procs, scoring='neg_mean_squared_error')\n",
    "    mean_mse[model] = -1.0*np.mean(neg_mse)\n",
    "    cv_std[model] = np.std(neg_mse)\n",
    "\n",
    "def print_summary(model, mean_mse, cv_std):\n",
    "    print('\\nModel:\\n', model)\n",
    "    print('Average MSE:\\n', mean_mse[model])\n",
    "    print('Standard deviation during CV:\\n', cv_std[model])\n",
    "\n",
    "def save_results(model, mean_mse, predictions, feature_importances):\n",
    "    '''saves model, model summary, feature importances, and predictions'''\n",
    "    with open('model.txt', 'w') as file:\n",
    "        file.write(str(model))\n",
    "    feature_importances.to_csv('feature_importances.csv') \n",
    "    np.savetxt('predictions.csv', predictions, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Encoding data\n"
     ]
    }
   ],
   "source": [
    "#define inputs\n",
    "train_feature_file = 'data/train_features.csv'\n",
    "train_target_file = 'data/train_salaries.csv'\n",
    "test_feature_file = 'data/test_features.csv'\n",
    "\n",
    "#define variables\n",
    "categorical_vars = ['companyId', 'jobType', 'degree', 'major', 'industry']\n",
    "numeric_vars = ['yearsExperience', 'milesFromMetropolis']\n",
    "target_var = 'salary'\n",
    "\n",
    "#load data\n",
    "print(\"Loading data\")\n",
    "feature_df = load_file(train_feature_file)\n",
    "target_df = load_file(train_target_file)\n",
    "test_df = load_file(test_feature_file)\n",
    "\n",
    "#consolidate training data\n",
    "raw_train_df = consolidate_data(feature_df, target_df, key='jobId')\n",
    "\n",
    "#clean, shuffle, and reindex training data -- shuffling improves cross-validation accuracy\n",
    "clean_train_df = shuffle(clean_data(raw_train_df)).reset_index()\n",
    "\n",
    "#encode categorical data and get final feature dfs\n",
    "print(\"Encoding data\")\n",
    "feature_df = one_hot_encode_feature_df(clean_train_df, cat_vars=categorical_vars, num_vars=numeric_vars)\n",
    "test_df = one_hot_encode_feature_df(test_df, cat_vars=categorical_vars, num_vars=numeric_vars)\n",
    "\n",
    "#get target df\n",
    "target_df = get_target_df(clean_train_df, target_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize model list and dicts\n",
    "models = []\n",
    "mean_mse = {}\n",
    "cv_std = {}\n",
    "res = {}\n",
    "\n",
    "#define number of processes to run in parallel\n",
    "num_procs = 2\n",
    "\n",
    "#shared model paramaters\n",
    "verbose_lvl = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning cross validation\n",
      "\n",
      "Model:\n",
      " LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
      "         normalize=False)\n",
      "Average MSE:\n",
      " 384.496112262743\n",
      "Standard deviation during CV:\n",
      " 0.407751472781797\n",
      "\n",
      "Model:\n",
      " Pipeline(memory=None,\n",
      "     steps=[('standardscaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('linearregression', LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
      "         normalize=False))])\n",
      "Average MSE:\n",
      " 384.4929406361134\n",
      "Standard deviation during CV:\n",
      " 0.4052140943158804\n",
      "\n",
      "Model:\n",
      " RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=25,\n",
      "           max_features=30, max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "           min_impurity_split=None, min_samples_leaf=1,\n",
      "           min_samples_split=60, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=150, n_jobs=2, oob_score=False, random_state=None,\n",
      "           verbose=0, warm_start=False)\n",
      "Average MSE:\n",
      " 367.5952539571058\n",
      "Standard deviation during CV:\n",
      " 0.033407548481477534\n"
     ]
    }
   ],
   "source": [
    "#create models -- hyperparameter tuning already done by hand for each model\n",
    "lr = LinearRegression()\n",
    "lr_std_pca = make_pipeline(StandardScaler(), PCA(), LinearRegression())\n",
    "rf = RandomForestRegressor(n_estimators=150, n_jobs=num_procs, max_depth=25, min_samples_split=60, \\\n",
    "                           max_features=30, verbose=verbose_lvl)\n",
    "gbm = GradientBoostingRegressor(n_estimators=150, max_depth=5, loss='ls', verbose=verbose_lvl)\n",
    "                      \n",
    "models.extend([lr, lr_std_pca, rf, gbm])\n",
    "\n",
    "#parallel cross-validate models, using MSE as evaluation metric, and print summaries\n",
    "print(\"Beginning cross validation\")\n",
    "for model in models:\n",
    "    train_model(model, feature_df, target_df, num_procs, mean_mse, cv_std)\n",
    "    print_summary(model, mean_mse, cv_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose model with lowest mse\n",
    "model = min(mean_mse, key=mean_mse.get)\n",
    "print('\\nPredictions calculated using model with lowest MSE:')\n",
    "print(model)\n",
    "\n",
    "#train model on entire dataset\n",
    "model.fit(feature_df, target_df)\n",
    "\n",
    "#create predictions based on test data\n",
    "predictions = model.predict(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'feature_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-1b57a759e010>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#linear models don't have feature_importances_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mimportances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mfeature_importances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'feature'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mfeature_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'importance'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mimportances\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'feature_df' is not defined"
     ]
    }
   ],
   "source": [
    "#store feature importances\n",
    "if hasattr(model, 'feature_importances_'):\n",
    "    importances = model.feature_importances_\n",
    "else:\n",
    "    #linear models don't have feature_importances_\n",
    "    importances = [0]*len(feature_df.columns)\n",
    "    \n",
    "feature_importances = pd.DataFrame({'feature':feature_df.columns, 'importance':importances})\n",
    "feature_importances.sort_values(by='importance', ascending=False, inplace=True)\n",
    "#set index to 'feature'\n",
    "feature_importances.set_index('feature', inplace=True, drop=True)\n",
    "\n",
    "#save results\n",
    "save_results(model, mean_mse[model], predictions, feature_importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'feature_importances' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-e260cf9a9990>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfeature_importances\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'feature_importances' is not defined"
     ]
    }
   ],
   "source": [
    "feature_importances[0:25].plot.bar(figsize=(20,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: model.txt: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!cat model.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head: feature_importances.csv: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!head feature_importances.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head: predictions.csv: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!head predictions.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
